---
title: "Class 08: Breast Cancer Analysis Project"
author: "Shazreh Hassan (PID: A13743949)"
format: pdf
toc: true
---

## Background

The goal of this mini-project is to explore a complete analysis using the unsupervised learning techniques covered in class. We’ll extend what we’ve learned by combining PCA as a preprocessing step to clustering using data that consist of measurements of cell nuclei of human breast masses.

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set first reported by K. P. Benne and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”.

Values in this data set describe characteristics of the cell nuclei present in digitized images of a fine needle aspiration (FNA) of a breast mass.


## Data import

```{r}
fna.data <- "https://bioboot.github.io/bimm143_S20/class-material/WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)

head(wisc.df)
```

We used row.names=1 so that patient id would not be included. We also want to remove diagnosis column:
```{r}
diagnosis <-  as.factor(wisc.df$diagnosis)
wisc.data <- wisc.df[,-1]
dim(wisc.data)
```

```{r}
head(wisc.data)
```
## Exploratory data analysis

>Q1. How many observations are in the dataset?

There are `r nrow(wisc.data)` observations

>Q2. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```
There are 212 malignant diagnoses

>Q3. How many variables/features in the data are suffixed with _mean?

```{r}
#colnames(wisc.data)

grep("_mean", colnames(wisc.data))  #returns column indices

length(grep("_mean", colnames(wisc.data)))
```

## Principal component analysis

The main function covered in base R for PCA is called `prcomp()`. We want to **scale and center** our data before PCA to make sure each feature is contributing equally to the analysis, preventing variables with larger variance from taking over in analysis. The optional argument `scale` should almost always be switched to `scale=TRUE`.

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
wisc.pr.summary <-summary(wisc.pr)
wisc.pr.summary
```

>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

0.4427 (about 44%)

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7

## Interpreting PCA Results

```{r}
biplot(wisc.pr)
```
>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

The plot is difficult to understand because of all the words overlapped onto each other.


Let's make our main result figure - the "PC plot" or "score plot", or "ordination plot"

```{r}
library(ggplot2)

ggplot(wisc.pr$x) +
  aes(PC1, PC2, col=diagnosis) +
  geom_point()
```

>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
ggplot(wisc.pr$x) +
  aes(PC1, PC3, col=diagnosis) +
  geom_point()
```

These plots seem to show the benign and malignant diagnoses in separable distributions across PC1. There is slightly more overlap in subgroups in the PC1 vs PC3 plot, which makes sense because PC2 explains more variance in the data than PC3.

## Variance explained

```{r}
#calculate the variance of each component - standard deviation squared

pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. Assign this to a variable called pve and create a plot of variance explained for each principal component.

```{r}
# Variance explained by each principal component: pve
pve <- pr.var/sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC.

```{r}
wisc.pr$rotation["concave.points_mean",1]
```

>Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

The model has 4 clusters at a height of **19**.

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)

#calculate euclidean distances
data.dist <- dist(data.scaled)

wisc.hclust <- hclust(data.dist, method="complete")

plot(wisc.hclust)
abline(h=19, col="red", lty=2)

```

## Selecting number of clusters

```{r}
#cut the tree so that it has 4 clusters
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)

#compare the cluster membership to the actual diagnoses
table(wisc.hclust.clusters, diagnosis)

```

Try it with a different number of clusters:

```{r}
wisc.hclust.newclusters <- cutree(wisc.hclust, k=2)

table(wisc.hclust.newclusters, diagnosis)
```

>Q11. OPTIONAL: Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10? How do you judge the quality of your result in each case?

It seems like 4 clusters is the best number, decreasing from 4 leads to more mixing (less separability) between the benign and malignant subgroups within clusters, while increasing from 4 leads to extra clusters with low numbers of observations regardless of subgroup.

>Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

PCA gave me results that made more sense, given that with clustering it looked like two of the clusters were very close together and 2 of the 4 clusters has very little observations compared to the other 2.


## Combining PCA and Clustering

```{r}
#only putting the first 3 PCs into the clustering
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="ward.D2")
plot(wisc.pr.hclust)
abline(h=70, col="red")
```

```{r}
grps <- cutree(wisc.pr.hclust, h=70)
table(grps)
```

>Q13. How well does the newly created model with four clusters separate out the two diagnoses?

Make a wee "cross-table"
```{r}
#compare cluster groups to diagnosis
table(grps, diagnosis)
```
TP: 179
FP: 24

**Sensitivity**: TP/(TP+FN)


>Q14. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

```{r}
table(wisc.hclust.clusters, diagnosis)
```
The clustering models before PCA did worse than the one that combined methods.


>Q16. Which of these new patients should we prioritize for follow up based on your results?

Patient 2

